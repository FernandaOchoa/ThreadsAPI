{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos con Text Analytics\n",
    "By Fernanda Ochoa.\n",
    "\n",
    "El presente notebook, desarrolla un ejemplo de como utilizar la data extraída por ThreadsAPI con la implementación de ```getData.py``` y el notebook de limpieza de datos ```threads.ipynb``` para trabajarlo con los servicios cognitivos de Azure.\n",
    "\n",
    "[Azure Cognitive Services](https://learn.microsoft.com/es-MX/azure/cognitive-services/) es una plataforma basada en la nube que ofrece servicios de inteligencia artificial (IA). Estos servicios nos permiten integrar capacidades cognitivas en aplicaciones sin necesidad de tener conocimientos profundos de IA o ciencia de datos.\n",
    "\n",
    "\n",
    "#### Categorías de servicios cognitivos\n",
    "\n",
    "Los servicios cognitivos se pueden clasificar en cinco áreas principales:\n",
    "\n",
    "+ [Voz](https://learn.microsoft.com/es-MX/azure/cognitive-services/): \n",
    "  + Voz a texto\n",
    "  + Texto a voz\n",
    "  + Traducción de voz\n",
    "  + Reconocimiento de voz\n",
    "+ [Lenguaje](https://learn.microsoft.com/es-MX/azure/cognitive-services/):\n",
    "  + Reconocimiento de entidades\n",
    "  + Análisis de opiniones\n",
    "  + Respuestas a preguntas\n",
    "  + Reconocimiento de lenguaje conversacional\n",
    "  + Traductor\n",
    "+ [Visión](https://learn.microsoft.com/es-MX/azure/cognitive-services/):\n",
    "  + Análisis de Imágenes y Video\n",
    "  + Modelos personalizados\n",
    "+ [Decisión](https://learn.microsoft.com/es-MX/azure/cognitive-services/):\n",
    "  + Detector de anomalías\n",
    "  + Moderador de contenido\n",
    "  + Personalizer\n",
    "+ [Servicio Azure OpenAI](https://learn.microsoft.com/es-MX/azure/cognitive-services/openai/concepts/models):\n",
    "  + Dall E\n",
    "  + GPT-4\n",
    "  + GPT-3.5\n",
    "  + Incrustaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Herramientas\n",
    "\n",
    "* Pandas\n",
    "    * Es una biblioteca considerada una extensión de Numpy para la manipulación y análisis de datos con Python.\n",
    "    * [Pandas Docs](https://pandas.pydata.org/)\n",
    "* Azure AI TextAnalytics:\n",
    "  * Es un servicio basado en la nube que proporciona funciones de procesamiento de lenguaje natural (NLP) para comprender y analizar texto.\n",
    "  * Es necesaria una suscripción de Azure para obtener las credenciales como ```APIKey``` y ```Url Endpoint```.\n",
    "  * [Azure AI TextAnalytics Docs](https://pypi.org/project/azure-ai-textanalytics/)\n",
    "* Dot env:\n",
    "  * Biblioteca para leer los pares clave-valor de un archivo .env y  establecerlos como variables de entorno. Ayuda en el desarrollo de aplicaciones siguiendo los principios de 12 factores.\n",
    "  * [Dot env Docs](https://pypi.org/project/python-dotenv/)\n",
    "\n",
    "#### Requisitos\n",
    "* Dataset: Conjunto de datos en csv con el que vamos a trabajar.\n",
    "  * Puedes encontrar un ejemplo en la carpeta: ```data``` en formato ```csv```.\n",
    "* Suscripción de Azure: Ingresar al [portal de azure](https://portal.azure.com), crear una suscripción, agregar una tarjeta (usamos la tarifa gratuita por lo que no hay costos) y listo.\n",
    "* [Activar suscripción](https://azure.microsoft.com/es-mx/free/)\n",
    "* Credenciales text analytics: \n",
    "  * Ingresar al [portal de azure](https://portal.azure.com)\n",
    "  * [Seguir el tutorial](https://learn.microsoft.com/es-mx/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Canomaly-detector%2Clanguage-service%2Ccomputer-vision%2Cwindows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalamos las Bibliotecas\n",
    "\n",
    "* [Text analytics](https://pypi.org/project/azure-ai-textanalytics/):\n",
    "  * Visual Studio Code: ```%pip install azure-ai-textanalytics```\n",
    "  * Google Colab: ```!pip install azure-ai-textanalytics```\n",
    "* [Dot env](https://pypi.org/project/python-dotenv/): \n",
    "  * Visual Studio Code: ```%pip install python-dotenv```\n",
    "  * Google Colab: ```!pip install python-dotenv```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credenciales\n",
    "\n",
    "Una vez creados los servicios cognitivos en el [portal de azure](https://portal.azure.com). Se debe crear en la raíz del proyecto un archivo llamado ```.env```, este archivo de configuración, contiene las variables de entorno del proyecto, es decir las credenciales de acceso al API de los servicios cognitivos.\n",
    "\n",
    "``` Contenido de mi archivo .env\n",
    "API_KEY = 'Reemplazar por tus credenciales'\n",
    "API_ENDPOINT = 'Reemplazar por tus credenciales'\n",
    "API_REGION = 'Reemplazar por tus credenciales'\n",
    "```\n",
    "\n",
    "Recuerda crear el archivo: ```.gitignore``` y añadir tu archivo ```.env``` si planeas subirlo a un repositorio de git.\n",
    "\n",
    "``` Contenigo de mi archivo .gitignore\n",
    "# Ignorar archivo .env que puede contener información sensible\n",
    ".env\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos requests\n",
    "%pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: azure-ai-textanalytics in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (5.3.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from azure-ai-textanalytics) (1.28.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from azure-ai-textanalytics) (1.1.28)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from azure-ai-textanalytics) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from azure-ai-textanalytics) (4.7.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos azure-ai-textanalytics\n",
    "%pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /Users/fernandaochoa/Library/Python/3.9/lib/python/site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos python-dotenv\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos las bibliotecas y variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para importar las variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas de Azure: Credentials y TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la biblioteca pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las credenciales del API\n",
    "endpoint = os.getenv('API_ENDPOINT')\n",
    "apiKey = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticamos el cliente\n",
    "client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(apiKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos nuestro csv con la data\n",
    "data = pd.read_csv('../data/cleanData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructuramos el dataframe con las columnas de interés: Text y Likes\n",
    "data.columns = ['Thread', 'Likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la longitud de cada texto\n",
    "data['len'] = data['Thread'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos la posición de la columna len al lado de la columna de texto del thread, obteniendo el nombre de la columna en la posición 2\n",
    "columnToMove = data.columns[2]\n",
    "\n",
    "col = data.pop(columnToMove)\n",
    "\n",
    "data.insert(1, columnToMove, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thread</th>\n",
       "      <th>len</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amanecimos con buenas noticias 💪🏽Ya podemos es...</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listo, ya solo documento algunos ejemplos con ...</td>\n",
       "      <td>231</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Thread  len  Likes\n",
       "0  Amanecimos con buenas noticias 💪🏽Ya podemos es...  141     14\n",
       "1  Listo, ya solo documento algunos ejemplos con ...  231     33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos el cambio de lugar de columnas\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el dataframe a una lista \n",
    "textThreads = data[\"Thread\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista para guardar los sentimientos de las revisiones\n",
    "sentiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamos el texto por lotes de 10 (límite de la API)\n",
    "for i in range(0, len(textThreads), 10):\n",
    "    batch = textThreads[i : i + 10]\n",
    "    result = client.analyze_sentiment(batch, show_opinion_mining=True)\n",
    "    docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    for idx, doc in enumerate(docs):\n",
    "        sentiments.append(doc.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si la cantidad de threads no son un múltiplos de 10, es posible que la longitud de\n",
    "# 'sentiments' sea menor que la longitud de 'threads'. En este caso, agregamos como sentimiento\n",
    "# desconocido para los threads restantes.\n",
    "while len(sentiments) < len(textThreads):\n",
    "    sentiments.append(\"desconocido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la columna Sentiment\n",
    "data['Sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thread</th>\n",
       "      <th>len</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amanecimos con buenas noticias 💪🏽Ya podemos es...</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listo, ya solo documento algunos ejemplos con ...</td>\n",
       "      <td>231</td>\n",
       "      <td>33</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya les ando preparando el repo para que puedan...</td>\n",
       "      <td>183</td>\n",
       "      <td>11</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cómo va su sábado?, ¿Ya tomaron café?</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Si llego a mil seguidores el fin les explico c...</td>\n",
       "      <td>215</td>\n",
       "      <td>31</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Thread  len  Likes Sentiment\n",
       "0  Amanecimos con buenas noticias 💪🏽Ya podemos es...  141     14  positive\n",
       "1  Listo, ya solo documento algunos ejemplos con ...  231     33   neutral\n",
       "2  Ya les ando preparando el repo para que puedan...  183     11   neutral\n",
       "3             ¿Cómo va su sábado?, ¿Ya tomaron café?   38     15   neutral\n",
       "4  Si llego a mil seguidores el fin les explico c...  215     31   neutral"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos el dataframe con la columna Sentiment\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si así lo deseamos, podemos exportarlo a un csv, complementarlo con más api's, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads API: Análisis de Sentimientos con Azure y Threads API\n",
    "\n",
    "Material desarrollado por: Fernanda Ochoa.\n",
    "\n",
    "Contacto:\n",
    "* GitHub: [FernandaOchoa](https://github.com/FernandaOchoa)\n",
    "* Twitter: [@imonsh](https://twitter.com/imonsh)\n",
    "* Instagram: [fherz8a](https://instagram.com/fherz8a)\n",
    "* Linkedin: [Fernanda Ochoa](https://www.linkedin.com/in/fernandaochoa8/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
